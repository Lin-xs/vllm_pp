{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e181ed0f43461d94378d1f70208832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"facebook/opt-30b\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OPTModel(\n",
       "  (decoder): OPTDecoder(\n",
       "    (embed_tokens): Embedding(50272, 7168, padding_idx=1)\n",
       "    (embed_positions): OPTLearnedPositionalEmbedding(2050, 7168)\n",
       "    (final_layer_norm): LayerNorm((7168,), eps=1e-05, elementwise_affine=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x OPTDecoderLayer(\n",
       "        (self_attn): OPTAttention(\n",
       "          (k_proj): Linear(in_features=7168, out_features=7168, bias=True)\n",
       "          (v_proj): Linear(in_features=7168, out_features=7168, bias=True)\n",
       "          (q_proj): Linear(in_features=7168, out_features=7168, bias=True)\n",
       "          (out_proj): Linear(in_features=7168, out_features=7168, bias=True)\n",
       "        )\n",
       "        (activation_fn): ReLU()\n",
       "        (self_attn_layer_norm): LayerNorm((7168,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=7168, out_features=28672, bias=True)\n",
       "        (fc2): Linear(in_features=28672, out_features=7168, bias=True)\n",
       "        (final_layer_norm): LayerNorm((7168,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "try:\n",
    "    from vllm.engine.llm_engine import exit_pipeline\n",
    "except ImportError:\n",
    "    pass\n",
    "try:\n",
    "    from vllm.utils import ctx_get_inteval_datetime\n",
    "except ImportError:\n",
    "    pass\n",
    "import time\n",
    "import argparse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-03 16:23:28 llm_engine.py:72] Initializing an LLM engine with config: model='facebook/opt-6.7b', tokenizer='facebook/opt-6.7b', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 104.06 MiB is free. Process 43827 has 38.20 GiB memory in use. Including non-PyTorch memory, this process has 9.21 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 1.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/v-yuanqwang/vllm_pp/test.ipynb 单元格 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.190.175.139/home/v-yuanqwang/vllm_pp/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sampling_params \u001b[39m=\u001b[39m SamplingParams(temperature\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.190.175.139/home/v-yuanqwang/vllm_pp/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                                     top_p\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.190.175.139/home/v-yuanqwang/vllm_pp/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                                     max_tokens\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.190.175.139/home/v-yuanqwang/vllm_pp/test.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m llm \u001b[39m=\u001b[39m LLM(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/opt-6.7b\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/vllm_pp/vllm/entrypoints/llm.py:93\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdisable_log_stats\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     78\u001b[0m engine_args \u001b[39m=\u001b[39m EngineArgs(\n\u001b[1;32m     79\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     80\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     92\u001b[0m )\n\u001b[0;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_engine \u001b[39m=\u001b[39m LLMEngine\u001b[39m.\u001b[39;49mfrom_engine_args(engine_args)\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_counter \u001b[39m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/vllm_pp/vllm/engine/llm_engine.py:231\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    228\u001b[0m distributed_init_method, placement_group \u001b[39m=\u001b[39m initialize_cluster(\n\u001b[1;32m    229\u001b[0m     parallel_config)\n\u001b[1;32m    230\u001b[0m \u001b[39m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49mengine_configs,\n\u001b[1;32m    232\u001b[0m              distributed_init_method,\n\u001b[1;32m    233\u001b[0m              placement_group,\n\u001b[1;32m    234\u001b[0m              log_stats\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m engine_args\u001b[39m.\u001b[39;49mdisable_log_stats)\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/vllm_pp/vllm/engine/llm_engine.py:110\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, distributed_init_method, placement_group, log_stats)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_workers_ray(placement_group)\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_workers(distributed_init_method)\n\u001b[1;32m    112\u001b[0m \u001b[39m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_cache()\n",
      "File \u001b[0;32m~/vllm_pp/vllm/engine/llm_engine.py:142\u001b[0m, in \u001b[0;36mLLMEngine._init_workers\u001b[0;34m(self, distributed_init_method)\u001b[0m\n\u001b[1;32m    134\u001b[0m worker \u001b[39m=\u001b[39m Worker(\n\u001b[1;32m    135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config,\n\u001b[1;32m    136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     distributed_init_method,\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers\u001b[39m.\u001b[39mappend(worker)\n\u001b[0;32m--> 142\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_workers(\n\u001b[1;32m    143\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    144\u001b[0m     get_all_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    145\u001b[0m )\n",
      "File \u001b[0;32m~/vllm_pp/vllm/engine/llm_engine.py:702\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, get_all_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m         executor \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(worker, method)\n\u001b[0;32m--> 702\u001b[0m     output \u001b[39m=\u001b[39m executor(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m     all_outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[1;32m    705\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_config\u001b[39m.\u001b[39mworker_use_ray:\n",
      "File \u001b[0;32m~/vllm_pp/vllm/worker/worker.py:70\u001b[0m, in \u001b[0;36mWorker.init_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m# Initialize the model.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m set_random_seed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config\u001b[39m.\u001b[39mseed)\n\u001b[0;32m---> 70\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m get_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_config)\n",
      "File \u001b[0;32m~/vllm_pp/vllm/model_executor/model_loader.py:95\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config)\u001b[0m\n\u001b[1;32m     93\u001b[0m     model \u001b[39m=\u001b[39m model_class(model_config\u001b[39m.\u001b[39mhf_config, quant_config)\n\u001b[1;32m     94\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     model \u001b[39m=\u001b[39m model_class(model_config\u001b[39m.\u001b[39;49mhf_config)\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m model_config\u001b[39m.\u001b[39mload_format \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     97\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/vllm_pp/vllm/model_executor/models/opt.py:281\u001b[0m, in \u001b[0;36mOPTForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    280\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m--> 281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m OPTModel(config)\n\u001b[1;32m    282\u001b[0m \u001b[39m# TODO(zhuohan): create a new weight after implementing pipeline\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39m#                parallelism\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head_weight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39membed_tokens\u001b[39m.\u001b[39mweight\n",
      "File \u001b[0;32m~/vllm_pp/vllm/model_executor/models/opt.py:262\u001b[0m, in \u001b[0;36mOPTModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config: OPTConfig):\n\u001b[1;32m    261\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder \u001b[39m=\u001b[39m OPTDecoder(config)\n",
      "File \u001b[0;32m~/vllm_pp/vllm/model_executor/models/opt.py:222\u001b[0m, in \u001b[0;36mOPTDecoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(\n\u001b[0;32m--> 222\u001b[0m     [OPTDecoderLayer(config) \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(config\u001b[39m.\u001b[39;49mnum_hidden_layers)])\n",
      "File \u001b[0;32m~/vllm_pp/vllm/model_executor/models/opt.py:222\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(\n\u001b[0;32m--> 222\u001b[0m     [OPTDecoderLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n",
      "File \u001b[0;32m~/vllm_pp/vllm/model_executor/models/opt.py:126\u001b[0m, in \u001b[0;36mOPTDecoderLayer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation_fn \u001b[39m=\u001b[39m get_act_fn(config\u001b[39m.\u001b[39mactivation_function)\n\u001b[1;32m    123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\n\u001b[1;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim,\n\u001b[1;32m    125\u001b[0m     elementwise_affine\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_elementwise_affine)\n\u001b[0;32m--> 126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m ColumnParallelLinear(\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim,\n\u001b[1;32m    128\u001b[0m     config\u001b[39m.\u001b[39;49mffn_dim,\n\u001b[1;32m    129\u001b[0m     bias\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49menable_bias,\n\u001b[1;32m    130\u001b[0m     gather_output\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2 \u001b[39m=\u001b[39m RowParallelLinear(\n\u001b[1;32m    133\u001b[0m     config\u001b[39m.\u001b[39mffn_dim,\n\u001b[1;32m    134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim,\n\u001b[1;32m    135\u001b[0m     bias\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39menable_bias,\n\u001b[1;32m    136\u001b[0m     input_is_parallel\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m )\n\u001b[1;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_layer_norm \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(\n\u001b[1;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim,\n\u001b[1;32m    140\u001b[0m     elementwise_affine\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mlayer_norm_elementwise_affine)\n",
      "File \u001b[0;32m~/vllm_pp/vllm/model_executor/parallel_utils/layers.py:138\u001b[0m, in \u001b[0;36mColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, quant_config)\u001b[0m\n\u001b[1;32m    133\u001b[0m     params_dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mget_default_dtype()\n\u001b[1;32m    135\u001b[0m \u001b[39m# Parameters.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# NOTE: torch.nn.functional.linear performs XA^T + b and as a result\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m# we allocate the transpose.\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_weights(params_dtype)\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(\n\u001b[1;32m    142\u001b[0m         torch\u001b[39m.\u001b[39mempty(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size_per_partition,\n\u001b[1;32m    143\u001b[0m                     device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mcurrent_device(),\n\u001b[1;32m    144\u001b[0m                     dtype\u001b[39m=\u001b[39mparams_dtype))\n",
      "File \u001b[0;32m~/vllm_pp/vllm/model_executor/parallel_utils/layers.py:150\u001b[0m, in \u001b[0;36mColumnParallelLinear.create_weights\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_weights\u001b[39m(\u001b[39mself\u001b[39m, dtype: torch\u001b[39m.\u001b[39mdtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(\n\u001b[0;32m--> 150\u001b[0m         torch\u001b[39m.\u001b[39;49mempty(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size_per_partition,\n\u001b[1;32m    151\u001b[0m                     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_size,\n\u001b[1;32m    152\u001b[0m                     device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mcurrent_device(),\n\u001b[1;32m    153\u001b[0m                     dtype\u001b[39m=\u001b[39;49mdtype))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 104.06 MiB is free. Process 43827 has 38.20 GiB memory in use. Including non-PyTorch memory, this process has 9.21 GiB memory in use. Of the allocated memory 8.76 GiB is allocated by PyTorch, and 1.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8,\n",
    "                                    top_p=0.95,\n",
    "                                    max_tokens=4)\n",
    "\n",
    "llm = LLM(model=\"facebook/opt-6.7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7088db52ea242ee867ee5aba0e5cc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"facebook/opt-13b\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304, 312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408, 416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504, 512]\n"
     ]
    }
   ],
   "source": [
    "ls = list(range(8, 512+1, 8))\n",
    "print(ls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm_my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
